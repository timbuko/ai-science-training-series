{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with CNNs\n",
    "\n",
    "Author: Varuni Sastry(vsastry@anl.gov), adapting codes from Bethany Lusch, Prasanna Balprakash, Corey Adams, and Kyle Felker\n",
    "\n",
    "In this notebook, we'll continue the MNIST problem using the Keras API (as included in the TensorFlow library) and incorporating convolutional layers.\n",
    "\n",
    "First, we will import the required library and frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set\n",
    "\n",
    "Again we'll load the MNIST handwritten digits data set. If you haven't downloaded it already, it could take a while. Once we download the train and test data, we normalize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(numpy.float32)\n",
    "x_test  = x_test.astype(numpy.float32)\n",
    "\n",
    "x_train /= 255.\n",
    "x_test  /= 255.\n",
    "\n",
    "y_train = y_train.astype(numpy.int32)\n",
    "y_test  = y_test.astype(numpy.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we won't flatten the images. \n",
    "\n",
    "The training data (`X_train`) is a 3rd-order tensor of size (60000, 28, 28), i.e. it consists of 60000 images of size 28x28 pixels. \n",
    "\n",
    "`y_train` is a 60000-dimensional vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network (CNN)\n",
    "\n",
    "CNN is a type of deep learning model for processing data that has a grid pattern, such as images. They are very similar to any other neural network having built with neurons that have learnable weights and biases. Each neuron receives a input, then a dot product with the weights and bias followed with some non-linear operation. The network still represents a function mapping from raw input (pixels in case of images) to a class score. All is good for small resolution images. What happens when the image is 256x256x3 ? The very first hiddden layer in a fully connected neural network will have 196K parameters!. So fully connected neural networks do not scale well for images. \n",
    "\n",
    "A typical convolutional neural network architecture consists of :\n",
    "* Convolutional Layer,\n",
    "* Pooling Layer, and \n",
    "* Fully-Connected Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conv layer](images/convNN_BlockDiagram.jpeg)\n",
    "Image credit: [Sumit Saha](https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a small model that includes convolutional layers\n",
    "\n",
    "- The Conv2D layers operate on 2D matrices so we input the digit images directly to the model.\n",
    "    - The two Conv2D layers belows learn 32 and 64 filters respectively. \n",
    "    - They are learning filters for 3x3 windows.\n",
    "- The MaxPooling2D layer reduces the spatial dimensions, that is, makes the image smaller.\n",
    "    - It downsamples by taking the maximum value in the window \n",
    "    - The pool size of (2, 2) below means the windows are 2x2. \n",
    "    - Helps in extracting important features and reduce computation\n",
    "- The Flatten layer flattens the 2D matrices into vectors, so we can then switch to Dense layers as in the MLP model.\n",
    "\n",
    "See https://keras.io/layers/convolutional/, https://keras.io/layers/pooling/ for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, activation=tf.nn.tanh):\n",
    "        tf.keras.models.Model.__init__(self)\n",
    "\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(32, [3, 3], activation='relu')\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(64, [3, 3], activation='relu')\n",
    "        self.pool_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.drop_4 = tf.keras.layers.Dropout(0.25)\n",
    "        self.dense_5 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.drop_6 = tf.keras.layers.Dropout(0.5)\n",
    "        self.dense_7 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool_3(x)\n",
    "        x = self.drop_4(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = self.dense_5(x)\n",
    "        x = self.drop_6(x)\n",
    "        x = self.dense_7(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Basics\n",
    "\n",
    "Convolution layer is formed with convolving a filter (usually 5x5 or 3x3) repeatedly over the input image to create a feature map, meaning the filter slides over spatially from the top left corner of the image to the bottom right corner of the image. Filters are the ones that learn diffenent features and detect the patterns in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conv layer](images/conv_layer.png)\n",
    "Image credit: [Jason Brownlee](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conv layer](images/conv.png)\n",
    "Image credit: [Anh H. Reynolds](https://anhreynolds.com/blogs/cnn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       \n",
    "### Padding\n",
    "Add zeros along the corners of the image, to preserve the dimensionality of the input. \n",
    "\n",
    "<img src=\"images/Padding.png\" width=\"300\" height=\"300\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is the output size ?**\n",
    "\n",
    "( N - F + 2P) / S  + 1\n",
    "\n",
    "where \n",
    "* N = dimension of the input image. (ex, for an image of 28x28x1, N=28)\n",
    "* F = dimension of the filter (F=3 for a filter of 3x3)\n",
    "* S = Stride value\n",
    "* P = Size of the zero padding used\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "\n",
    "Pooling reduces the dimensionality of the images, with max-pooling being one of the most widely used.\n",
    "\n",
    "<img src=\"images/MaxpoolSample2.png\" width=\"600\" hight=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Channels\n",
    "\n",
    "Usually colored images have multiple channels with RGB values, what happens to the filter sizes and activation map dimensions in those cases ? \n",
    "\n",
    "<img src=\"images/multiple_channels.png\" width=\"600\" height=\"600\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature visualization of cnns \n",
    "\n",
    "The filters from the initial hidden layers tend to learn low level features like edges, corners, shapes, colors etc. Filters from the deeper networks tend to learn high level features detecting patterns like wheel, car, etc. \n",
    "\n",
    "<img src=\"images/convnets-feature-maps.png\" width=\"600\" height=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a concise way to train the network, like we did in the previous notebook. We'll see a more verbose approach below that allows more performance tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_concise(_batch_size, _n_training_epochs, _lr):\n",
    "\n",
    "    cnn_model = MNISTClassifier()\n",
    "\n",
    "    cnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    x_train_reshaped = numpy.expand_dims(x_train, -1)\n",
    "    history = cnn_model.fit(x_train_reshaped, y_train, batch_size=_batch_size, epochs=_n_training_epochs)\n",
    "    return history, cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This took 35-47 seconds over the different epochs on google colab\n",
    "batch_size = 512\n",
    "epochs = 3\n",
    "lr = .01\n",
    "history, cnn_model = train_network_concise(batch_size, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for test data.  The model should be better than the non-convolutional model even if you're only patient enough for three epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['accuracy'])\n",
    "plt.title('accuracy');\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With enough training epochs, the test accuracy should exceed 99%.\n",
    "\n",
    "You can compare your result with the state-of-the art [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html). Even more results can be found [here](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_test_reshaped = numpy.expand_dims(x_test, -1)\n",
    "scores = cnn_model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (cnn_model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also again check the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Confusion matrix (rows: true classes; columns: predicted classes):'); print()\n",
    "predictions = cnn_model.predict(x_test_reshaped)\n",
    "cm=confusion_matrix(y_test, numpy.argmax(predictions, axis=1), labels=list(range(10)))\n",
    "print(cm); print()\n",
    "\n",
    "print('Classification accuracy for each class:'); print()\n",
    "for i,j in enumerate(cm.diagonal()/cm.sum(axis=1)): print(\"%d: %.4f\" % (i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More verbose training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach explicitly handles the looping over data. It will be helpful this afternoon for diving in and optimizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    # if labels are integers, use sparse categorical crossentropy\n",
    "    # network's final layer is softmax, so from_logtis=False\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    # if labels are one-hot encoded, use standard crossentropy\n",
    "\n",
    "    return scce(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, batch_data, y_true):\n",
    "    y_pred = model(batch_data)\n",
    "    loss = compute_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function that will manage the training loop for us:\n",
    "\n",
    "def train_loop(batch_size, n_training_epochs, model, opt):\n",
    "    \n",
    "    @tf.function()\n",
    "    def train_iteration(data, y_true, model, opt):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = forward_pass(model, data, y_true)\n",
    "\n",
    "        trainable_vars = model.trainable_variables\n",
    "\n",
    "        # Apply the update to the network (one at a time):\n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        opt.apply_gradients(zip(grads, trainable_vars))\n",
    "        return loss\n",
    "\n",
    "    for i_epoch in range(n_training_epochs):\n",
    "        print(\"beginning epoch %d\" % i_epoch)\n",
    "        start = time.time()\n",
    "\n",
    "        epoch_steps = int(60000/batch_size)\n",
    "        dataset.shuffle(60000) # Shuffle the whole dataset in memory\n",
    "        batches = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        \n",
    "        for i_batch, (batch_data, y_true) in enumerate(batches):\n",
    "            batch_data = tf.reshape(batch_data, [-1, 28, 28, 1])\n",
    "            loss = train_iteration(batch_data, y_true, model, opt)\n",
    "            \n",
    "        end = time.time()\n",
    "        print(\"took %1.1f seconds for epoch #%d\" % (end-start, i_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(_batch_size, _n_training_epochs, _lr):\n",
    "\n",
    "    mnist_model = MNISTClassifier()\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(_lr)\n",
    "\n",
    "    train_loop(_batch_size, _n_training_epochs, mnist_model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset.shuffle(60000)\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 3\n",
    "lr = .01\n",
    "train_network(batch_size, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced networks\n",
    "\n",
    "### ResNet\n",
    "\n",
    "Deeper and deeper networks that stack convolutions end up with smaller and smaller gradients in early layers. ResNets use additional skip connections where the output layer H(x) from being f(w x + b) or f(x) to be f(x) + x. This avoids vanishing gradient problem and results in smoother loss functions. Refer [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf) and [ResNet loss visualization paper](https://arxiv.org/pdf/1712.09913.pdf) for more information.\n",
    " \n",
    "![ResNet](images/ResNet.png) Image credit: [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Nets \n",
    "U-NET is a convolution based neural network architecture orginally developed for biomedical image segmentation tasks. It has an encoder-decoder architecture with skip connections in between them. \n",
    "\n",
    "![U-Nets](images/U-Nets.png) Image credit: [ResNet paper](https://arxiv.org/pdf/1505.04597.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTs\n",
    "\n",
    "CNNs are prone to translation invariance and a locally restricted receptive field. Transformers are used instead, but they are operate on sequences! So the images are split into patches and flattened. Use linear embeddings and positional embeddings and feed it to a encoder based treansformer model. \n",
    "![ViTs](images/ViT.gif) Image credit: [Google Blog](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
